まさにそのとおり。
**LightGBM／XGBoostに“そこそこ良い特徴量”** までは全員が到達します。
スコアを抜きん出させるのは、その “さらに一歩先” の工夫です。

---

## baselineの一歩先で差が付くポイント

| レバー           | 典型的アプローチ            | もう一段深掘りすると…                                                                 |
| ------------- | ------------------- | --------------------------------------------------------------------------- |
| **交差検証**      | K-Fold（5 or 10）     | *Stratified*・*Group*・*TimeSeriesCV*　→　リーク防止 × 分散低減                          |
| **特徴量**       | 対数変換／平方根／多項式        | ❶ *カテゴリ×数値* 交互作用<br>❷ *順位・比率* 系（例：価格÷地域平均）<br>❸ *集約特徴量*（例：ユーザー単位 mean, std） |
| **エンコーディング**  | OHE／LabelEnc        | ❶ *Target Encoding*（K-fold 平均）<br>❷ *CatBoost Encoding*（リークガード付き）           |
| **ハイパーパラメータ** | ランダムサーチ／Bayes       | ❶ *Optuna + early pruning*<br>❷ *学習率 warm start* で高速探索                      |
| **アンサンブル**    | RF × LGBM × XGB の平均 | ❶ *Stacking*（メタモデルは ElasticNet or L2 LGBM）<br>❷ *時間・ID* で重み可変 Blending      |
| **モデル安定化**    | デフォルト種別             | ❶ *KFold seed averaging*（seed 10 個）<br>❷ *Bagging\_freq* を 1-5 に            |
| **再現性**       | 手元スクリプトのみ           | ❶ *dbt* で前処理テスト → “データ割付” の揺れをゼロ化<br>❷ *MLflow* でパラメ・メトリクス自動ログ              |

---

### すばやく差をつける小ネタ

1. **“木” の落穂拾い**
   *任意の 2 モデルで予測が大きく食い違うサンプル* を抽出 →
   その行だけ追加特徴量を作る “局所 FE” が効くことあり。

2. **計算量ソート CV**
   テストデータのサイズが大きいなら、

   * モデルA（軽い）で全テスト予測
   * モデルB（重い）は *上位 30 %* だけ再予測
     → 推論時間制限のあるコンペで有利。

3. **Feature Importance に頼り過ぎない**
   木モデルの Gain/TOTAL は “似た特徴量を食い合う” ので
   *Permutation Importance* と *SHAP* 両方で確認。
   不要だと確信した列を潔く削ると安定度↑。

---

### 要は **「試行回数 × 品質管理」**

* **LightGBM/XGB** は“強いが飽和しやすい”
  → *似たパラメ & 特徴量* でも前処理のブレで 0.01pt 動く
* **品質管理（dbt, MLflow）** が入ると
  → ブレを抑えつつ **試行回数を倍に** 出せる
  → 同じアルゴリズムでも **メダル圏に滑り込む確率** が跳ね上がる

> **ベースラインは皆同じ。**
> そこから“ブレを減らす作業”と“もう一歩の特徴量”を
> **どれだけ早く・多く** 試せるかで勝負が決まります。
